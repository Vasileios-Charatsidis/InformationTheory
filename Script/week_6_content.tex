\section{The converse}
...

\section{Joint Typicality}
In order to prepare for the ncc theorem.

\begin{definition}[Joint typicality]
For a joint distribution $P_{XY}$, define the jointly typical set $\typset$ as
\begin{align*}
\typset := \{(x^n,y^n) \in \mathcal{X}^n \times \mathcal{Y}^n &\mid | - \frac{1}{n} \log P_{X^n}(x^n) - H(X)| < \epsilon,\\
&\mid | - \frac{1}{n} \log P_{Y^n}(y^n) - H(Y)| < \epsilon,\\
&\mid | - \frac{1}{n} \log P_{X^nY^n}(x^n,y^n) - H(XY)| < \epsilon\}
\end{align*}
where $P_{X^n}$ is just $\prod_{i=1}^n P_X$.
\end{definition}
So also the joint surprisal should be close to the joint entropy.

\begin{theorem}[Joint Asymptotic Equipartition Property]
Let $X^nY^n \stackrel{iid}{\sim} P_{XY}$, then
\begin{enumerate}
\item $P_{X^nY^n}(\typset) \convto 1$.
\item $|\typset| \leq 2^{n(H(XY) + \epsilon)}$
\item If $\tilde{X}^n\tilde{Y}^n \stackrel{iid}{\sim} P_X \cdot P_Y$, then $P[(\tilde{X}^n\tilde{Y}^n) \in \typset] \leq 2^{-n(I(X;Y) - 3\epsilon)}$.
\item For large enough $n$, $P[(\tilde{X}^n\tilde{Y}^n) \in \typset] \geq (1-\epsilon)2^{-n(I(X;Y) + 3 \epsilon)}$.
\end{enumerate}
\end{theorem}
Extremes: if $X$ and $Y$ are independent, everything is jointly typical. Then (3) is upper bounded by 1, basically.
If $X = Y$, then the mutual information is $H(X)$, and the jointly typical set is going to be quite small (only everything that is on the diagonal).

\begin{proof}
Using weak law of large numbers.
\begin{enumerate}
\item $-\frac{1}{n} \log P_{X^n}(X^n) = - \frac{1}{n} \sum_{i=1}^n \log P_X(x) \convtoP - \mathbb{E}[\log P_X(X)] = H(X)$. Same argument as in regular AEP. For $\epsilon > 0$, there exists an $n_1$ such that for all $n \geq n_1$,
\[
P[|-\frac{1}{n} \log P_{X^n}(X^n) - H(X) \geq \epsilon] < \epsilon/3.
\]
Similarly, there exists an $n_2$ such that for all $n \geq n_2$,
\[
P[|-\frac{1}{n} \log P_{Y^n}(Y^n) - H(Y) \geq \epsilon] < \epsilon/3.
\]
and there exists an $n_3$ such that for all $n \geq n_3$,
\[
P[|-\frac{1}{n} \log P_{X^nY^n}(X^nY^n) - H(XY) \geq \epsilon] < \epsilon/3.
\]
Then choose $\tilde{n} = \max\{n_1,n_2,n_3\}$, then for all $n \geq \tilde{n}$, the three conditions hold. Then by union bound,
\[
P[X^nY^n \not\in \typset] < \epsilon.
\]
\item $1 = \sum_{(x^ny^n) \in \mathcal{X}^n \times \mathcal{Y}^n} P_{X^nY^n}(x^ny^n) \geq ...$
\item see:
\begin{align*}
P[(\tilde{X}^n\tilde{Y}^n) \in \typset] &= \sum_{(\tilde{x}^n\tilde{y}^n) \in \typset)} P_{X^n}(x^n) \times P_{Y^n}(y^n)\\
&\leq |\typset| 2^{-n(H(X) - \epsilon)}2^{-n(H(Y) - \epsilon} \mbox(because both probs are upper bounded)\\
&\leq 2^{n(H(XY) + \epsilon}2^{-n(H(X) - \epsilon)}2^{-n(H(Y) - \epsilon} \mbox{by (2)}\\
&= 2^{-n(-H(XY) +H(X) + H(Y) - 3\epsilon}\\
&= 2^{-n(I(X;Y) - 3\epsilon)}
\end{align*}
\item see:
\begin{align*}
P[(\tilde{X}^n\tilde{Y}^n) \in \typset] &= \sum_{(\tilde{x}^n\tilde{y}^n) \in \typset)} P_{X^n}(x^n) \times P_{Y^n}(y^n)\\
&\geq |\typset| 2^{-n(H(X) + \epsilon)}2^{-n(H(Y) + \epsilon}\\
&\geq (1-\epsilon)2^{n(H(XY) - \epsilon}2^{-n(H(X) + \epsilon)}2^{-n(H(Y) + \epsilon}\\
&= (1-\epsilon)2^{-n(I(X;Y) + 3\epsilon)}
\end{align*}

\end{enumerate}
\end{proof}

\section{Shannon's Noisy Channel Coding Theorem}

All rates below capacity are achievable
\begin{theorem}[Shannon's Noisy Channel Coding Theorem]
$\forall \epsilon > 0 \ \forall R < C \ \exists n$ such that there exists a $(\lceil 2^{nR}\rceil, n)$ code with $\lambda^{(n)} < \epsilon$.
\end{theorem}