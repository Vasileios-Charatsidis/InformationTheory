\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{2}
\newcommand\deadline{Friday November 18th, 20:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\usepackage{tikz}

\begin{document}

\homeworkproblems

{\sffamily\noindent
%This week's exercises deal with sets, counting and uniform probabilities.
Your homework must be handed in \textbf{electronically via Moodle before \deadline}. This deadline is strict and late submissions are graded with a 0. At the end of the course, the lowest of your 6 weekly homework grades will be dropped. You are strongly encouraged to work together on the exercises, including the homework. However, after this discussion phase, you have to write down and submit your own individual solution. Numbers alone are never sufficient, always motivate your answers.
}

\begin{exercise}[Deriving the weak law of large numbers (7pt)]
	\begin{subex}[(3pt)] (Markov's inequality) For any real non-negative random variable $X$ and any $t > 0$, show that
	\[
	P_X(X \geq t) \leq \frac{\mathbb{E}[X]}{t}\, .
	\]
	Exhibit a random variable (which can depend on $t$) that achieves this inequality with equality.
	\end{subex}
	\begin{subex}[(2pt)] (Chebyshev's inequality.) Let $Y$ be a random variable with mean $\mu$ and variance $\sigma^2$. By letting $X = (Y - \mu)^2$, show that for any $\varepsilon > 0$,
	\[
	P(|Y - \mu| > \varepsilon) \leq \frac{\sigma^2}{\varepsilon^2} \, .
	\]
	\end{subex}
	\begin{subex}[(2pt)] (The weak law of large numbers.) Let $Z_1, Z_2, ..., Z_n$ real i.i.d. random variables with mean $\mu = \mathbb{E}[Z_i]$ and variance $\sigma^2 = \mathbb{E}[(X_i - \mu)^2] < \infty$. Define the random variables $S_n = \frac{1}{n} \sum_{i=1}^n Z_i$. Show that
	\[
	P(|S_n - \mu | > \varepsilon) \leq \frac{\sigma^2}{n\varepsilon^2}\, .
	\]
	Thus, $P(|S_n - \mu| > \varepsilon) \to 0$ as $n \to \infty$. This is known as the weak law of large numbers (Theorem 2.6.1 in the lecture notes).
	\end{subex}
\end{exercise}

\begin{exercise}[Huffman Coding (7pt)]
	\begin{subex}[(4pt)]
	For a binary source $P_X$ with $P_X(0) = \frac{1}{8}$ and $P_X(1) = \frac{7}{8}$, design a Huffman code for blocks of $N = 1,2$ and 3 bits. For each of the three codes, compute the average codeword length and divide it by $N$, in order to compare it to the optimal length, i.e. the entropy of the source. What do you observe?
	\end{subex}
	\begin{subex}[(1pt)]
	If you were asked in (a) to design a Huffman code for a block of $N = 100$ bits, what problem would you run into?
	\end{subex}
	\begin{subex}[(2pt)]
	Consider the random variable $Z$ with
	\begin{center}
	\begin{tabular}{c | c c c c c c}
	$z$ & 1 & 2 & 3 & 4 & 5 & 6\\
	\hline
	$P_Z(z)$ & 1/10 & 3/10 & 2/10 & 2/10 & 1/10 & 1/10\\
	\end{tabular}
	\end{center}
	Find an optimal \emph{ternary} Huffman encoding for $Z$ (i.e. using an alphabet with three symbols).
	\end{subex}
\end{exercise}



\begin{exercise}[Inefficiency when using the wrong code (6pt)]
Given are two distributions $P_X$ and $Q_X$ for the same set $\mathcal{X} = \{\mathtt{a,b,c,d}\}$:
\[
\begin{array}{c | c c c c}
x & \mathtt{a}&\mathtt{b}&\mathtt{c}&\mathtt{d}\\\hline
P_X(x) & 1/4&1/4&1/4&1/4\\
Q_X(x) & 1/2&1/4&1/8&1/8\\
\end{array}
\]
\begin{subex}[(2pt)]
Design an optimal code for $Q_X(x)$. What is its expected codeword length? What is the expected codeword length if you use this code to encode the source $P_X$?
\end{subex}
\begin{subex}[(4pt)]
Give an upper \emph{and} lower bound for the expected codeword length $\mathbb{E}_{P_X}(\ell(X))$ if you encode the source $P_X$ using a code with codeword lengths $\ell(X)$ that are optimal for the source $Q_X$, i.e. $\ell(X) = \lceil -\log Q_X(X)\rceil$. Write the bounds in terms of $D(P_X||Q_X)$, the relative entropy between $P_X$ and $Q_X$.
\end{subex}
\end{exercise}






\end{document}