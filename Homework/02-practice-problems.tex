\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{2}
\newcommand\deadline{Friday November 18th, 20:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\begin{document}

\practiceproblems

{\sffamily\noindent
%This week's exercises deal with sets, counting and uniform probabilities.
This week's exercises deal with ...

You do not have to hand in these exercises, they are for practicing only. Problems marked with a $\bigstar$ are generally a bit harder. If you have questions about any of the exercises, please post them in the discussion forum on Moodle, and try to help each other. We will also keep an eye on the forum.
%Your homework must be handed in \textbf{electronically via Moodle before \deadline}. This deadline is strict and late submissions are graded with a 0. At the end of the course, the lowest of your 7 weekly homework grades will be dropped. You are strongly encouraged to work together on the exercises, including the homework. However, after this discussion phase, you have to write down and submit your own individual solution. Numbers alone are never sufficient, always motivate your answers.
}



\begin{exercise}[Maximal conditional entropy implies independence]
(remove)
	\begin{subex}
	Prove that if $H(X|Y) = \log(|\mathcal{X}|)$ then $X$ and $Y$ are independent.
	\end{subex}
	
	\begin{subex}
	Give a joint distribution $P_{XY}$ where $H(X) = \log(|\mathcal{X}|)$, but $X$ and $Y$ are dependent.
	\end{subex}
\end{exercise}

\begin{exercise}[Kraft's inequality]
(concept) Below, six binary codes are shown for the source symbols $x_1, ..., x_4$.
\begin{center}
\begin{tabular}{l | l l l l l l}
&Code A& Code B & Code C & Code D & Code E & Code F\\
\hline
$x_1$ & 00 & 0    & 0     &  0     & 1    & 1\\
$x_2$ & 01 & 10  & 11   & 100  & 01  & 10\\
$x_3$ & 10 & 11  & 100 & 110  & 001 & 100\\
$x_4$ & 11 & 110 & 110 & 111 & 0001 & 1000\\
\end{tabular}
\end{center}
	\begin{subex}[(1pt)]
	Which codes fulfill the Kraft inequality?
	\end{subex}
	\begin{subex}[(1pt)]
	Is a code that satisfies this inequality always uniquely decodable?
	\end{subex}
	\begin{subex}[(1pt)]
	Which codes are prefix-free?
	\end{subex}
	\begin{subex}[(1pt)]
	Which codes are uniquely decodable?
	\end{subex}
\end{exercise}

\begin{exercise}[]
(practice) Something with binary entropy - the Stirling approximation? (andersom met 2 tot de macht ipv ln) What does this mean for the relation between binomial coefficient and the binary entropy function
\end{exercise}

\begin{exercise}[An optimal code]
(Practice)
(MacKay 5.25) Let $X$ be a random variable. Show that if for all $x \in \mathcal{X}$, there is some $n \in \mathbb{N}$ such that $P_X(x) = \frac{1}{2^n}$, then there exists a source code whose expected length equals the entropy. 
\end{exercise}


\begin{exercise}[Geometric distribution]
(practice) Compute the entropy of the geometric distribution. (do with trees)
\end{exercise}

\begin{exercise}[Mackay, example 2.13]
(practice) see homework 1 from 2014
\end{exercise}

\end{document}